Metadata-Version: 2.4
Name: ai-eval-harness
Version: 0.1.0
Summary: A minimal AI eval harness with CI gates and reproducible datasets.
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: openai>=1.0.0
Requires-Dist: pydantic>=2.6.0
Requires-Dist: rapidfuzz>=3.6.0
Requires-Dist: jsonschema>=4.21.0
Provides-Extra: dev
Requires-Dist: pytest>=8.0.0; extra == "dev"
Requires-Dist: ruff>=0.6.0; extra == "dev"

# AI Eval Harness

A minimal, reproducible evaluation harness for LLM prompt tasks.

## What this repo demonstrates
- Reproducible eval datasets (`.jsonl`)
- CI gating (fail PRs on quality regressions)
- Deterministic unit tests (no API required)
- Optional live LLM eval job in GitHub Actions (runs only when `OPENAI_API_KEY` is set)

## Local dev
```bash
python -m venv .venv && source .venv/bin/activate
pip install -e .[dev]
pytest -q
ruff check . && ruff format --check .

# Live eval (requires OPENAI_API_KEY)
export OPENAI_API_KEY="..."
python -m eval_harness.runner --dataset datasets/classification.jsonl --model gpt-5.2 --min-pass-rate 0.90
```

## GitHub Actions
- `test` job always runs (lint + unit tests)
- `evals` job runs only when repo secrets include `OPENAI_API_KEY`
